#!/usr/bin/env python3
"""
ìŠ¤ë§ˆíŠ¸ ë¹Œë”© ì—ë„ˆì§€ ê´€ë¦¬ ì‹œìŠ¤í…œ (SBEMS) - ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ê°œë°œ
ìµœì í™”ëœ ì „ì²˜ë¦¬ ë°ì´í„°ë¥¼ í™œìš©í•œ ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import lightgbm as lgb

# ì‹œê³„ì—´ íŠ¹í™” ëª¨ë¸
from sklearn.ensemble import IsolationForest
import joblib
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SmartBuildingMLModels:
    """ìŠ¤ë§ˆíŠ¸ ë¹Œë”© ì—ë„ˆì§€ ì˜ˆì¸¡ ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self, data_path='data/processed/preprocessed_building_data.csv'):
        self.data_path = data_path
        self.models = {}
        self.model_scores = {}
        self.feature_importance = {}
        self.best_model = None
        self.scaler = StandardScaler()
        
    def load_and_prepare_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ëª¨ë¸ í•™ìŠµ ì¤€ë¹„"""
        logger.info("ë°ì´í„° ë¡œë“œ ë° ëª¨ë¸ í•™ìŠµ ì¤€ë¹„ ì¤‘...")
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
        self.df = pd.read_csv(self.data_path)
        
        # ìˆ˜ì¹˜í˜• íŠ¹ì„±ë§Œ ì„ íƒ
        numeric_features = self.df.select_dtypes(include=[np.number]).columns.tolist()
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬
        self.target = 'power_consumption'
        if self.target in numeric_features:
            numeric_features.remove(self.target)
        
        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        self.X = self.df[numeric_features]
        self.y = self.df[self.target]
        
        # ë¬´í•œê°’ ì²˜ë¦¬
        self.X = self.X.replace([np.inf, -np.inf], np.nan)
        self.X = self.X.fillna(self.X.median())
        
        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í•  (ì‹œê³„ì—´ ê³ ë ¤)
        train_size = int(len(self.X) * 0.8)
        self.X_train = self.X[:train_size]
        self.X_test = self.X[train_size:]
        self.y_train = self.y[:train_size]
        self.y_test = self.y[train_size:]
        
        # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        
        logger.info(f"ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {self.X_train.shape[0]} í›ˆë ¨, {self.X_test.shape[0]} í…ŒìŠ¤íŠ¸")
        logger.info(f"íŠ¹ì„± ê°œìˆ˜: {self.X_train.shape[1]}")
        
        return self.X_train, self.X_test, self.y_train, self.y_test
    
    def evaluate_model(self, model, X_train, X_test, y_train, y_test, model_name):
        """ëª¨ë¸ í‰ê°€"""
        # í›ˆë ¨ ë° ì˜ˆì¸¡
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        # í‰ê°€ ì§€í‘œ ê³„ì‚°
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        # ìƒëŒ€ ì˜¤ì°¨ ê³„ì‚°
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
        
        # êµì°¨ ê²€ì¦
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
        
        results = {
            'RMSE': rmse,
            'MAE': mae,
            'R2': r2,
            'MAPE': mape,
            'CV_R2_mean': cv_scores.mean(),
            'CV_R2_std': cv_scores.std()
        }
        
        logger.info(f"{model_name} ì„±ëŠ¥:")
        logger.info(f"  RMSE: {rmse:.4f}")
        logger.info(f"  MAE: {mae:.4f}")
        logger.info(f"  RÂ²: {r2:.4f}")
        logger.info(f"  MAPE: {mape:.2f}%")
        logger.info(f"  CV RÂ²: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
        
        return model, results, y_pred
    
    def train_linear_models(self):
        """ì„ í˜• ëª¨ë¸ í›ˆë ¨"""
        logger.info("=== ì„ í˜• ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ===")
        
        # 1. Linear Regression
        logger.info("Linear Regression í›ˆë ¨ ì¤‘...")
        lr_model, lr_results, lr_pred = self.evaluate_model(
            LinearRegression(), self.X_train_scaled, self.X_test_scaled, 
            self.y_train, self.y_test, "Linear Regression"
        )
        self.models['Linear_Regression'] = lr_model
        self.model_scores['Linear_Regression'] = lr_results
        
        # 2. Ridge Regression
        logger.info("Ridge Regression í›ˆë ¨ ì¤‘...")
        ridge_model, ridge_results, ridge_pred = self.evaluate_model(
            Ridge(alpha=1.0), self.X_train_scaled, self.X_test_scaled, 
            self.y_train, self.y_test, "Ridge Regression"
        )
        self.models['Ridge_Regression'] = ridge_model
        self.model_scores['Ridge_Regression'] = ridge_results
        
        # 3. Lasso Regression
        logger.info("Lasso Regression í›ˆë ¨ ì¤‘...")
        lasso_model, lasso_results, lasso_pred = self.evaluate_model(
            Lasso(alpha=0.1), self.X_train_scaled, self.X_test_scaled, 
            self.y_train, self.y_test, "Lasso Regression"
        )
        self.models['Lasso_Regression'] = lasso_model
        self.model_scores['Lasso_Regression'] = lasso_results
        
        return lr_pred, ridge_pred, lasso_pred
    
    def train_ensemble_models(self):
        """ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨"""
        logger.info("=== ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ===")
        
        # 1. Random Forest
        logger.info("Random Forest í›ˆë ¨ ì¤‘...")
        rf_model, rf_results, rf_pred = self.evaluate_model(
            RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
            self.X_train, self.X_test, self.y_train, self.y_test, "Random Forest"
        )
        self.models['Random_Forest'] = rf_model
        self.model_scores['Random_Forest'] = rf_results
        self.feature_importance['Random_Forest'] = rf_model.feature_importances_
        
        # 2. Gradient Boosting
        logger.info("Gradient Boosting í›ˆë ¨ ì¤‘...")
        gb_model, gb_results, gb_pred = self.evaluate_model(
            GradientBoostingRegressor(n_estimators=100, random_state=42),
            self.X_train, self.X_test, self.y_train, self.y_test, "Gradient Boosting"
        )
        self.models['Gradient_Boosting'] = gb_model
        self.model_scores['Gradient_Boosting'] = gb_results
        self.feature_importance['Gradient_Boosting'] = gb_model.feature_importances_
        
        # 3. XGBoost
        logger.info("XGBoost í›ˆë ¨ ì¤‘...")
        xgb_model, xgb_results, xgb_pred = self.evaluate_model(
            xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1),
            self.X_train, self.X_test, self.y_train, self.y_test, "XGBoost"
        )
        self.models['XGBoost'] = xgb_model
        self.model_scores['XGBoost'] = xgb_results
        self.feature_importance['XGBoost'] = xgb_model.feature_importances_
        
        # 4. LightGBM
        logger.info("LightGBM í›ˆë ¨ ì¤‘...")
        lgb_model, lgb_results, lgb_pred = self.evaluate_model(
            lgb.LGBMRegressor(n_estimators=100, random_state=42, n_jobs=-1),
            self.X_train, self.X_test, self.y_train, self.y_test, "LightGBM"
        )
        self.models['LightGBM'] = lgb_model
        self.model_scores['LightGBM'] = lgb_results
        self.feature_importance['LightGBM'] = lgb_model.feature_importances_
        
        return rf_pred, gb_pred, xgb_pred, lgb_pred
    
    def train_svm_model(self):
        """SVM ëª¨ë¸ í›ˆë ¨"""
        logger.info("=== SVM ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ===")
        
        # SVM (ì»¤ë„ íŠ¸ë¦­ ì‚¬ìš©)
        logger.info("SVM í›ˆë ¨ ì¤‘...")
        svm_model, svm_results, svm_pred = self.evaluate_model(
            SVR(kernel='rbf', C=1.0, gamma='scale'),
            self.X_train_scaled, self.X_test_scaled, 
            self.y_train, self.y_test, "SVM"
        )
        self.models['SVM'] = svm_model
        self.model_scores['SVM'] = svm_results
        
        return svm_pred
    
    def hyperparameter_tuning(self, model_name='XGBoost'):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
        logger.info(f"=== {model_name} í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ===")
        
        if model_name == 'XGBoost':
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 5, 7],
                'learning_rate': [0.01, 0.1, 0.2],
                'subsample': [0.8, 0.9, 1.0],
                'colsample_bytree': [0.8, 0.9, 1.0]
            }
            base_model = xgb.XGBRegressor(random_state=42)
        
        elif model_name == 'Random_Forest':
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [10, 20, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
            base_model = RandomForestRegressor(random_state=42, n_jobs=-1)
        
        # Grid Search
        grid_search = GridSearchCV(
            base_model, param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=1
        )
        grid_search.fit(self.X_train, self.y_train)
        
        # ìµœì  ëª¨ë¸ í‰ê°€
        best_model = grid_search.best_estimator_
        best_model, best_results, best_pred = self.evaluate_model(
            best_model, self.X_train, self.X_test, 
            self.y_train, self.y_test, f"Tuned {model_name}"
        )
        
        logger.info(f"ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
        
        self.models[f'Tuned_{model_name}'] = best_model
        self.model_scores[f'Tuned_{model_name}'] = best_results
        
        return best_model, best_results, best_pred
    
    def compare_models(self):
        """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
        logger.info("=== ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ===")
        
        # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
        results_df = pd.DataFrame(self.model_scores).T
        
        # ì„±ëŠ¥ ìˆœìœ„
        results_df['Rank'] = results_df['R2'].rank(ascending=False)
        results_df = results_df.sort_values('Rank')
        
        logger.info("\nëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„:")
        for idx, (model_name, row) in enumerate(results_df.iterrows(), 1):
            logger.info(f"{idx:2d}. {model_name:<20} | RÂ²: {row['R2']:.4f} | RMSE: {row['RMSE']:.4f} | MAPE: {row['MAPE']:.2f}%")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
        best_model_name = results_df.index[0]
        self.best_model = self.models[best_model_name]
        
        logger.info(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
        logger.info(f"   RÂ² Score: {results_df.loc[best_model_name, 'R2']:.4f}")
        logger.info(f"   RMSE: {results_df.loc[best_model_name, 'RMSE']:.4f}")
        logger.info(f"   MAPE: {results_df.loc[best_model_name, 'MAPE']:.2f}%")
        
        return results_df
    
    def analyze_feature_importance(self, model_name='XGBoost'):
        """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""
        logger.info(f"=== {model_name} íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ===")
        
        if model_name in self.feature_importance:
            importance = self.feature_importance[model_name]
            feature_names = self.X_train.columns
            
            # ì¤‘ìš”ë„ ë°ì´í„°í”„ë ˆì„ ìƒì„±
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': importance
            }).sort_values('importance', ascending=False)
            
            # ìƒìœ„ 20ê°œ íŠ¹ì„± ì¶œë ¥
            logger.info("ìƒìœ„ 20ê°œ ì¤‘ìš” íŠ¹ì„±:")
            for i, (_, row) in enumerate(importance_df.head(20).iterrows(), 1):
                logger.info(f"{i:2d}. {row['feature']:<35} | {row['importance']:.4f}")
            
            return importance_df
        
        else:
            logger.warning(f"{model_name}ì˜ íŠ¹ì„± ì¤‘ìš”ë„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
    
    def save_models(self, output_dir='models/'):
        """ëª¨ë¸ ì €ì¥"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        logger.info("ëª¨ë¸ ì €ì¥ ì¤‘...")
        
        # ê° ëª¨ë¸ ì €ì¥
        for model_name, model in self.models.items():
            model_path = os.path.join(output_dir, f'{model_name}.pkl')
            joblib.dump(model, model_path)
            logger.info(f"  {model_name} ì €ì¥: {model_path}")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        scaler_path = os.path.join(output_dir, 'scaler.pkl')
        joblib.dump(self.scaler, scaler_path)
        logger.info(f"  Scaler ì €ì¥: {scaler_path}")
        
        # ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼ ì €ì¥
        results_df = pd.DataFrame(self.model_scores).T
        results_path = os.path.join(output_dir, 'model_performance.csv')
        results_df.to_csv(results_path)
        logger.info(f"  ì„±ëŠ¥ ê²°ê³¼ ì €ì¥: {results_path}")
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥
        if self.feature_importance:
            importance_df = self.analyze_feature_importance('XGBoost')
            if importance_df is not None:
                importance_path = os.path.join(output_dir, 'feature_importance.csv')
                importance_df.to_csv(importance_path, index=False)
                logger.info(f"  íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥: {importance_path}")
    
    def run_full_pipeline(self):
        """ì „ì²´ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("=== ìŠ¤ë§ˆíŠ¸ ë¹Œë”© ì—ë„ˆì§€ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
        
        # 1. ë°ì´í„° ì¤€ë¹„
        self.load_and_prepare_data()
        
        # 2. ì„ í˜• ëª¨ë¸ í›ˆë ¨
        self.train_linear_models()
        
        # 3. ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨
        self.train_ensemble_models()
        
        # 4. SVM ëª¨ë¸ í›ˆë ¨
        self.train_svm_model()
        
        # 5. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (XGBoost)
        self.hyperparameter_tuning('XGBoost')
        
        # 6. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
        results_df = self.compare_models()
        
        # 7. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        self.analyze_feature_importance('XGBoost')
        
        # 8. ëª¨ë¸ ì €ì¥
        self.save_models()
        
        logger.info("=== ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===")
        
        return results_df

if __name__ == "__main__":
    # ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    ml_pipeline = SmartBuildingMLModels()
    results = ml_pipeline.run_full_pipeline()
    
    print(f"\nğŸ‰ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ê°œë°œ ì™„ë£Œ!")
    print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {results.index[0]}")
    print(f"ğŸ“Š RÂ² Score: {results.iloc[0]['R2']:.4f}")
    print(f"ğŸ“ˆ RMSE: {results.iloc[0]['RMSE']:.4f}")
    print(f"ğŸ“‰ MAPE: {results.iloc[0]['MAPE']:.2f}%")
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: models/")


